# -*-coding:utf-8-*-
"""
Ragas è¯„ä¼°æ¨¡å—

è¯¥æ¨¡å—ä½¿ç”¨ Ragas æ¡†æ¶å¯¹ RAG ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œè¡¡é‡æ£€ç´¢å’Œç”Ÿæˆè´¨é‡ã€‚
è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š
    - faithfulness (å¿ å®åº¦): å›ç­”æ˜¯å¦å¿ äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    - answer_relevancy (ç­”æ¡ˆç›¸å…³æ€§): å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
    - context_precision (ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡): æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå°‘æ˜¯çœŸæ­£æœ‰ç”¨çš„
    - context_recall (ä¸Šä¸‹æ–‡å¬å›ç‡): éœ€è¦çš„ä¿¡æ¯è¢«æ£€ç´¢åˆ°äº†å¤šå°‘

ä½¿ç”¨æ–¹æ³•ï¼š
    python rag_qa/rag_assesment/rag_as.py
"""

# å¯¼å…¥ pandas åº“ï¼Œç”¨äºæ•°æ®å¤„ç†å’Œä¿å­˜ CSV æ–‡ä»¶
import pandas as pd
# å¯¼å…¥ json æ¨¡å—ï¼Œç”¨äºåŠ è½½è¯„ä¼°æ•°æ®
import json
# å¯¼å…¥ os æ¨¡å—ï¼Œç”¨äºè·¯å¾„æ“ä½œ
import os
import sys

# è®¾ç½®è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
rag_qa_path = os.path.dirname(current_dir)
sys.path.insert(0, rag_qa_path)
project_root = os.path.dirname(rag_qa_path)
sys.path.insert(0, project_root)

# å¯¼å…¥ ragas åº“çš„ evaluate å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œ RAG è¯„ä¼°
from ragas import evaluate
# å¯¼å…¥ ragas çš„è¯„ä¼°æŒ‡æ ‡
from ragas.metrics import (
    faithfulness,       # å¿ å®åº¦ï¼šå›ç­”æ˜¯å¦åŸºäºç»™å®šä¸Šä¸‹æ–‡
    answer_relevancy,   # ç­”æ¡ˆç›¸å…³æ€§ï¼šå›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
    context_precision,  # ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç²¾å‡†
    context_recall      # ä¸Šä¸‹æ–‡å¬å›ç‡ï¼šæ˜¯å¦æ£€ç´¢åˆ°äº†è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯
)
# å¯¼å…¥ datasets åº“çš„ Dataset ç±»ï¼Œç”¨äºæ„å»º RAGAS æ‰€éœ€çš„æ•°æ®æ ¼å¼
from datasets import Dataset
# å¯¼å…¥ langchain_community çš„ Ollama èŠå¤©æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œç”¨äºæœ¬åœ°æ¨¡å‹è°ƒç”¨
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings

# å¯¼å…¥é¡¹ç›®é…ç½®
from base import logger, Config


def load_evaluate_data(json_path: str) -> list:
    """
    åŠ è½½è¯„ä¼°æ•°æ®é›†
    
    Args:
        json_path: JSON æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«è¯„ä¼°æ•°æ®çš„åˆ—è¡¨
    """
    logger.info(f"æ­£åœ¨åŠ è½½è¯„ä¼°æ•°æ®: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è¯„ä¼°æ•°æ®")
    return data


def convert_to_ragas_format(data: list) -> Dataset:
    """
    å°†åŸå§‹æ•°æ®è½¬æ¢ä¸º Ragas è¦æ±‚çš„ Dataset æ ¼å¼
    
    Ragas è¦æ±‚çš„å­—æ®µï¼š
        - question: ç”¨æˆ·çš„é—®é¢˜
        - contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨ (æ³¨æ„æ˜¯å¤æ•°å½¢å¼)
        - answer: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
        - ground_truth: æ ‡å‡†ç­”æ¡ˆ/å‚è€ƒç­”æ¡ˆ
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        
    Returns:
        Ragas Dataset å¯¹è±¡
    """
    logger.info("æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼ä¸º Ragas Dataset...")
    
    # åˆå§‹åŒ–å„å­—æ®µåˆ—è¡¨
    questions = []
    contexts = []
    answers = []
    ground_truths = []
    
    # éå†æ¯æ¡æ•°æ®ï¼Œæå–å­—æ®µ
    for item in data:
        questions.append(item["question"])
        # Ragas è¦æ±‚ contexts æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬çš„æ•°æ®ä¸­ context æœ¬èº«å°±æ˜¯åˆ—è¡¨
        contexts.append(item["context"])
        answers.append(item["answer"])
        ground_truths.append(item["ground_truth"])
    
    # æ„å»º Ragas å…¼å®¹çš„å­—å…¸æ ¼å¼
    ragas_data = {
        "question": questions,
        "contexts": contexts,  # Ragas è¦æ±‚å¤æ•°å½¢å¼
        "answer": answers,
        "ground_truth": ground_truths
    }
    
    # è½¬æ¢ä¸º Hugging Face datasets.Dataset å¯¹è±¡
    dataset = Dataset.from_dict(ragas_data)
    logger.info(f"æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆï¼Œå…± {len(dataset)} æ¡è®°å½•")
    
    return dataset


def run_evaluation(dataset: Dataset, llm, embeddings) -> dict:
    """
    æ‰§è¡Œ Ragas è¯„ä¼°
    
    Args:
        dataset: Ragas Dataset å¯¹è±¡
        llm: ç”¨äºè¯„ä¼°çš„è¯­è¨€æ¨¡å‹
        embeddings: ç”¨äºè¯„ä¼°çš„åµŒå…¥æ¨¡å‹
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    logger.info("å¼€å§‹æ‰§è¡Œ Ragas è¯„ä¼°...")
    logger.info("è¯„ä¼°æŒ‡æ ‡: faithfulness, answer_relevancy, context_precision, context_recall")
    
    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
    metrics = [
        faithfulness,       # å¿ å®åº¦
        answer_relevancy,   # ç­”æ¡ˆç›¸å…³æ€§
        context_precision,  # ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡
        context_recall      # ä¸Šä¸‹æ–‡å¬å›ç‡
    ]
    
    # è°ƒç”¨ Ragas evaluate å‡½æ•°æ‰§è¡Œè¯„ä¼°
    result = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=llm,
        embeddings=embeddings
    )
    
    logger.info("Ragas è¯„ä¼°å®Œæˆ!")
    return result


def save_results(result, output_path: str):
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ° CSV æ–‡ä»¶
    
    Args:
        result: Ragas è¯„ä¼°ç»“æœ
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    logger.info(f"æ­£åœ¨ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
    
    # å°†ç»“æœè½¬æ¢ä¸º DataFrame
    df = result.to_pandas()
    
    # ä¿å­˜ä¸º CSV æ–‡ä»¶
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def print_summary(result):
    """
    æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
    
    Args:
        result: Ragas è¯„ä¼°ç»“æœ
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š Ragas è¯„ä¼°ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    # æ‰“å°å„æŒ‡æ ‡å¾—åˆ†
    for metric_name, score in result.items():
        if isinstance(score, float):
            # æ ¹æ®åˆ†æ•°ç»™å‡ºè¯„çº§æç¤º
            if score >= 0.8:
                emoji = "ğŸŸ¢"  # ä¼˜ç§€
                level = "ä¼˜ç§€"
            elif score >= 0.6:
                emoji = "ğŸŸ¡"  # è‰¯å¥½
                level = "è‰¯å¥½"
            elif score >= 0.4:
                emoji = "ğŸŸ "  # ä¸€èˆ¬
                level = "ä¸€èˆ¬"
            else:
                emoji = "ğŸ”´"  # éœ€æ”¹è¿›
                level = "éœ€æ”¹è¿›"
            
            print(f"{emoji} {metric_name}: {score:.4f} ({level})")
    
    print("=" * 60)
    print("\nğŸ“ æŒ‡æ ‡è¯´æ˜:")
    print("  - faithfulness: å›ç­”æ˜¯å¦å¿ äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ (è¶Šé«˜è¶Šå¥½)")
    print("  - answer_relevancy: å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ (è¶Šé«˜è¶Šå¥½)")
    print("  - context_precision: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æœ‰å¤šç²¾å‡† (è¶Šé«˜è¶Šå¥½)")
    print("  - context_recall: éœ€è¦çš„ä¿¡æ¯è¢«æ£€ç´¢åˆ°äº†å¤šå°‘ (è¶Šé«˜è¶Šå¥½)")
    print()


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„ Ragas è¯„ä¼°æµç¨‹
    """
    print("\nğŸš€ å¯åŠ¨ Ragas RAG è¯„ä¼°ç³»ç»Ÿ...\n")
    
    # ========== 1. åŠ è½½è¯„ä¼°æ•°æ® ==========
    json_path = os.path.join(current_dir, "rag_evaluate_data.json")
    data = load_evaluate_data(json_path)
    
    # ========== 2. è½¬æ¢ä¸º Ragas æ ¼å¼ ==========
    dataset = convert_to_ragas_format(data)
    
    # ========== 3. é…ç½®è¯„ä¼°æ¨¡å‹ ==========
    # ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹è¿›è¡Œè¯„ä¼°
    # æ³¨æ„ï¼šç¡®ä¿ Ollama æœåŠ¡å·²å¯åŠ¨ï¼Œä¸”å·²ä¸‹è½½ qwen2.5:7b æ¨¡å‹
    logger.info("æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹ (Ollama qwen2.5:7b)...")
    
    llm = ChatOllama(
        model="qwen2.5:7b",
        base_url='http://localhost:11434'
    )
    
    embeddings = OllamaEmbeddings(
        model="qwen2.5:7b",
        base_url='http://localhost:11434'
    )
    
    logger.info("è¯„ä¼°æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # ========== 4. æ‰§è¡Œè¯„ä¼° ==========
    result = run_evaluation(dataset, llm, embeddings)
    
    # ========== 5. æ‰“å°ç»“æœæ‘˜è¦ ==========
    print_summary(result)
    
    # ========== 6. ä¿å­˜è¯¦ç»†ç»“æœ ==========
    output_csv_path = os.path.join(current_dir, "ragas_evaluation_result.csv")
    save_results(result, output_csv_path)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_csv_path}")


if __name__ == "__main__":
    main()
