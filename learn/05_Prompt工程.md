# 知识点 5: Prompt 工程

> 📍 **核心文件**: `rag_qa/core/prompts.py`  
> ⏱️ **学习时间**: 约 20-30 分钟  
> 🎯 **重要性**: ⭐⭐⭐ (决定 LLM 输出质量)

---

## 🤔 核心问题：为什么 Prompt 工程很重要?

### Prompt 的威力

```
同样的 LLM，不同的 Prompt，完全不同的效果：

❌ 差的 Prompt：
"回答: {question}"
→ LLM: "我不知道具体信息，建议您咨询专业人士。"

✅ 好的 Prompt：
"你是一个智能助手。基于以下上下文回答问题。
上下文: {context}
问题: {question}
如果无法回答，请说明原因。"
→ LLM: "根据提供的文档，AI课程学费为19800元..."
```

**核心价值**：
- ✅ Prompt 是人和 LLM 的接口
- ✅ 好的 Prompt 能显著提升答案质量
- ✅ 成本低（无需微调模型）

---

## 第一部分：RAG 系统中的 Prompt 体系

### 📊 Prompt 分类

项目中有 **4 类 Prompt**：

| Prompt 类型 | 用途 | 调用时机 | 重要性 |
|------------|------|---------|--------|
| **RAG Prompt** | 最终答案生成 | 每次查询 | ⭐⭐⭐⭐⭐ |
| **HyDE Prompt** | 生成假设答案 | 使用 HyDE 策略时 | ⭐⭐⭐ |
| **子查询 Prompt** | 拆分复杂问题 | 使用子查询策略时 | ⭐⭐⭐ |
| **回溯问题 Prompt** | 简化复杂问题 | 使用回溯策略时 | ⭐⭐⭐ |

---

## 第二部分：RAG Prompt - 核心中的核心 ⭐⭐⭐⭐⭐

### 🎯 代码位置：第 9-26 行

```python
def rag_prompt():
    return PromptTemplate(
        template="""  
        你是一个智能助手，帮助用户回答问题。  
        如果提供了上下文，请基于上下文回答；如果没有上下文，请直接根据你的知识回答。  
        如果答案来源于检索到的文档，请在回答中说明。

        上下文: {context}  
        问题: {question}  

        如果无法回答，请回复："信息不足，无法回答，请联系人工客服，电话：{phone}。"  
        回答:  
        """,
        input_variables=["context", "question", "phone"],
    )
```

### 💡 设计原则解析

#### 1. 明确的角色定位

```
你是一个智能助手，帮助用户回答问题。
```

**为什么重要？**
- 设定 LLM 的"人设"
- 引导输出风格（专业、友好、简洁等）

**其他例子**：
- "你是一个教育咨询顾问..." → 输出更专业
- "你是一个幽默的助手..." → 输出更轻松

#### 2. 清晰的指令层次

```
如果提供了上下文，请基于上下文回答；
如果没有上下文，请直接根据你的知识回答。
```

**设计意图**：
- ✅ **优先使用检索结果**：避免 LLM 幻觉
- ✅ **兜底机制**：上下文为空时不傻站着
- ✅ **灵活性**：适配通用知识和专业咨询

#### 3. 输出格式引导

```
如果答案来源于检索到的文档，请在回答中说明。
```

**效果**：
```
✅ 好的输出：
"根据课程资料，AI课程学费为19800元..."

❌ 普通输出：
"AI课程学费为19800元。"
```

**价值**：
- 答案可追溯
- 增加用户信任度

#### 4. 明确的兜底策略

```
如果无法回答，请回复："信息不足，无法回答，请联系人工客服，电话：{phone}。"
```

**设计考虑**：
- ✅ **避免乱答**：没信息时不要编造
- ✅ **提供出口**：引导用户联系人工
- ✅ **参数化**：电话号码可配置

#### 5. 结构化输入

```
上下文: {context}  
问题: {question}  
```

**为什么分开？**
- ✅ LLM 能清晰区分背景信息和问题
- ✅ 便于调试（看到具体输入）
- ✅ 模板复用性强

---

## 第三部分：检索策略 Prompt 详解

### 🔍 HyDE Prompt（假设答案生成）

**代码位置**: 第 29-40 行

```python
def hyde_prompt():
    return PromptTemplate(
        template="""  
           假设你是用户，想了解以下问题，请生成一个简短的假设答案：  
           问题: {query}  
           假设答案:  
           """,
        input_variables=["query"],
    )
```

**设计要点**：

| 要素 | 内容 | 作用 |
|------|------|------|
| 角色转换 | "假设你是用户" | 换位思考，生成更真实的答案 |
| 明确任务 | "生成一个简短的假设答案" | 控制输出长度 |
| 输出引导 | "假设答案:" | 让 LLM 直接输出答案，不啰嗦 |

**实际效果**：
```
问题："人工智能在教育领域的应用有哪些？"

LLM 生成假设答案：
"人工智能在教育领域的应用包括智能辅导系统、自动批改作业、
个性化学习推荐、学习数据分析、虚拟教学助手等。"

→ 用这个假设答案去检索，比用原问题更容易找到相关文档
```

---

### 🔀 子查询 Prompt（问题拆分）

**代码位置**: 第 43-58 行

```python
def subquery_prompt():
    return PromptTemplate(
        template="""  
           将以下复杂查询分解为多个简单子查询，每行一个子查询，最多生成两个子查询（只保留子查询问题，其他的文本都不需要）：
           eg: 
           用户原始query："Milvus 和 Zilliz Cloud 在功能上有什么不同？
           子查询："Milvus 有哪些功能？"，"Zilliz Cloud 有哪些功能？"
           
           查询: {query}  
           子查询:  
           """,
        input_variables=["query"],
    )
```

**设计要点**：

#### 1. 明确的任务描述
```
将以下复杂查询分解为多个简单子查询
```

#### 2. 格式约束
```
每行一个子查询，最多生成两个子查询
```
**为什么限制数量？**
- ✅ 避免生成过多子查询（耗时长）
- ✅ 聚焦核心问题

#### 3. Few-shot 示例 ⭐
```
eg: 
用户原始query："Milvus 和 Zilliz Cloud 在功能上有什么不同？
子查询："Milvus 有哪些功能？"，"Zilliz Cloud 有哪些功能？"
```

**Few-shot Learning 的威力**：
```
无示例：
LLM 可能输出："1. 功能对比  2. 性能分析"（格式不对）

有示例：
LLM 输出："Milvus 有哪些功能？"，"Zilliz Cloud 有哪些功能？"（格式正确）
```

#### 4. 输出过滤指令
```
只保留子查询问题，其他的文本都不需要
```
**作用**：避免 LLM 输出解释性文字

---

### 🔄 回溯问题 Prompt（问题简化）

**代码位置**: 第 61-72 行

```python
def backtracking_prompt():
    return PromptTemplate(
        template="""  
           将以下复杂查询简化为一个更简单的问题：  
           查询: {query}  
           简化问题:  
           """,
        input_variables=["query"],
    )
```

**设计特点**：
- ✅ **极简设计**：任务明确，无需过多说明
- ✅ **单一输出**：只要一个简化后的问题
- ✅ **开放性**：给 LLM 充分的简化空间

**实际效果**：
```
原问题：
"我想学AI，但我数学不好，不知道能不能学会，
而且预算有限，想了解课程难度和学费情况，
还有学完后能不能找到工作..."

简化后：
"AI课程的难度和学费是多少？"
```

---

## 第四部分：Prompt 优化技巧 ⭐⭐⭐

### 🎨 优化技巧总结

#### 1. 角色定位（Role）

```
❌ 差：无角色
"回答以下问题: {question}"

✅ 好：明确角色
"你是一个教育咨询专家，帮助学生了解课程信息。
问题: {question}"
```

**影响**：
- 角色明确 → 输出风格一致
- 专业角色 → 答案更可信

---

#### 2. 上下文结构（Context）

```
❌ 差：混在一起
"根据以下信息回答问题: {context} {question}"

✅ 好：分开标注
"上下文: {context}
问题: {question}"
```

**影响**：
- 结构清晰 → LLM 理解更准确
- 便于调试 → 容易发现问题

---

#### 3. 输出格式（Format）

```
❌ 差：无格式要求
"回答问题"

✅ 好：明确格式
"请用以下格式回答：
1. 直接答案（1-2句话）
2. 详细解释（如需要）
3. 相关建议（如适用）"
```

**影响**：
- 格式统一 → 前端好解析
- 结构化 → 用户体验好

---

#### 4. 约束条件（Constraints）

```
❌ 差：无约束
"生成子查询"

✅ 好：明确约束
"生成子查询，要求：
1. 最多2个
2. 每行一个
3. 只保留问题，不要编号和解释"
```

**影响**：
- 避免冗余输出
- 后续处理简单

---

#### 5. Few-shot 示例（Examples）

```
❌ 差：无示例
"将问题拆分成子查询"

✅ 好：提供示例
"将问题拆分成子查询，例如：
输入: '比较A和B的优缺点'
输出: 'A有哪些优点？', 'B有哪些优点？'"
```

**影响**：
- LLM 更容易理解任务
- 输出格式更统一

---

## 第五部分：Prompt 调试与优化流程

### 🛠️ 调试流程

#### Step 1: 记录输入输出

```python
# 在 rag_system.py 中添加
logger.debug(f"Prompt 输入:\n{prompt_input}")
answer = self.llm(prompt_input)
logger.debug(f"LLM 输出:\n{answer}")
```

#### Step 2: 分析问题

**常见问题**：

| 问题 | 可能原因 | 解决方案 |
|------|---------|---------|
| 答案啰嗦 | 无输出格式约束 | 添加"简洁回答"提示 |
| 答案偏离主题 | 角色定位不明确 | 强化角色描述 |
| 格式不统一 | 缺少 few-shot 示例 | 添加示例 |
| 经常说"不知道" | Prompt 过于严格 | 放松约束条件 |

#### Step 3: A/B 测试

```python
# 测试两个版本的 Prompt
prompts = {
    "v1": "回答问题: {question}",
    "v2": "你是专家，基于上下文回答: {context}\n问题: {question}"
}

for version, template in prompts.items():
    answer = llm(template.format(...))
    print(f"{version}: {answer}")
```

#### Step 4: 迭代优化

```
版本1（baseline） → 版本2（加角色） → 版本3（加示例） → 版本4（优化格式）
```

---

## 第六部分：实战案例

### 案例 1：优化 RAG Prompt

**初始版本**：
```python
template = """
回答问题: {question}
参考: {context}
"""
```

**问题**：
- LLM 经常忽略 context
- 答案格式不统一

**优化版本 1**：
```python
template = """
你是一个智能助手。
基于以下参考信息回答问题。

参考信息: {context}
问题: {question}

要求：
1. 优先使用参考信息
2. 如果参考信息不足，请说明
3. 保持简洁
"""
```

**效果**：
- ✅ LLM 更关注 context
- ✅ 答案结构更清晰
- ⚠️ 但有时还是太啰嗦

**优化版本 2**（最终版本）：
```python
template = """
你是一个智能助手，帮助用户回答问题。  
如果提供了上下文，请基于上下文回答；如果没有上下文，请直接根据你的知识回答。  
如果答案来源于检索到的文档，请在回答中说明。

上下文: {context}  
问题: {question}  

如果无法回答，请回复："信息不足，无法回答，请联系人工客服，电话：{phone}。"  
回答:  
"""
```

**改进**：
- ✅ 添加兜底策略
- ✅ 参数化电话号码
- ✅ 明确指令优先级

---

### 案例 2：优化子查询 Prompt

**初始版本**：
```python
template = "把这个问题拆成几个小问题: {query}"
```

**问题**：
- LLM 输出格式混乱
- 有时生成太多子查询

**优化版本**：
```python
template = """
将以下复杂查询分解为多个简单子查询，每行一个子查询，最多生成两个子查询（只保留子查询问题，其他的文本都不需要）：
eg: 
用户原始query："Milvus 和 Zilliz Cloud 在功能上有什么不同？
子查询："Milvus 有哪些功能？"，"Zilliz Cloud 有哪些功能？"

查询: {query}  
子查询:  
"""
```

**改进**：
- ✅ 添加 few-shot 示例
- ✅ 限制数量（最多2个）
- ✅ 明确格式要求

---

## ✅ 核心概念检查清单

- [x] **Prompt 的重要性**：决定 LLM 输出质量
- [x] **角色定位**：明确 LLM 的"人设"
- [x] **指令分层**：优先级清晰
- [x] **格式引导**：统一输出格式
- [x] **兜底策略**：处理异常情况
- [x] **Few-shot Learning**：提供示例
- [x] **约束条件**：控制输出范围

---

## 📊 Prompt 设计清单

设计一个新 Prompt 时，检查以下要素：

```
□ 角色定位明确？
□ 任务描述清晰？
□ 输入变量定义？
□ 输出格式要求？
□ 约束条件完整？
□ 有 few-shot 示例？（如需要）
□ 兜底策略考虑？
□ 参数可配置？
```

---

## 🎓 进阶思考

### Q1: 如何判断 Prompt 好坏？

**量化指标**：
1. **准确性**：答案是否正确
2. **相关性**：是否基于检索结果
3. **简洁性**：是否冗长
4. **一致性**：相似问题输出格式是否一致

**测试方法**：
```python
# 准备测试集
test_cases = [
    {"query": "AI课程学费?", "expected": "包含具体金额"},
    {"query": "课程难度?", "expected": "基于文档回答"},
    ...
]

# 批量测试
for case in test_cases:
    answer = rag_system.generate_answer(case["query"])
    评估答案质量
```

### Q2: Prompt 工程 vs 微调（Fine-tuning）？

| 对比维度 | Prompt 工程 | 微调 |
|---------|------------|------|
| 成本 | 低（改文字） | 高（需要GPU训练） |
| 速度 | 快（秒级） | 慢（小时/天） |
| 灵活性 | 高（随时改） | 低（需重新训练） |
| 适用场景 | 多数情况 | 特定任务，大量数据 |

**建议**：
- ✅ 先尽量优化 Prompt
- ✅ Prompt 无法解决时，考虑微调

---

## 📚 扩展阅读

1. **Prompt Engineering Guide**: [https://www.promptingguide.ai/](https://www.promptingguide.ai/)
2. **OpenAI Best Practices**: [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)
3. **Few-shot Learning**: 搜索 "few-shot prompting techniques"

---

**上一个知识点**: [04_文档分割策略.md](./04_文档分割策略.md)  
**下一个知识点**: [06_查询分类器.md](./06_查询分类器.md)
