# Ragas RAG 评估框架使用指南

## 📖 什么是 Ragas？

**Ragas (RAG Assessment)** 是一个专门用于评估检索增强生成 (RAG) 系统的开源框架。它通过多个维度的指标，客观地衡量一个 RAG 系统的**检索质量**和**生成质量**。

简单来说，Ragas 能帮你回答以下问题：
- 我的 RAG 系统检索到的资料**对不对**？（检索质量）
- 我的 RAG 系统生成的答案**好不好**？（生成质量）

---

## 📊 四大核心评估指标

Ragas 提供了四个关键指标，从不同角度评估 RAG 系统的表现：

### 1. 🎯 Faithfulness (忠实度)

**含义**: 生成的答案是否**忠于检索到的上下文**，有没有"瞎编"。

**计算逻辑**:
- 如果答案中的每一个论断 (claim) 都能在检索到的上下文中找到依据，得分就高。
- 如果答案里出现了上下文里根本没提到的内容（幻觉 / Hallucination），得分就低。

**理想分数**: 越接近 **1.0** 越好。

**示例**:
- **上下文**: "Python 是一种解释型语言。"
- **回答**: "Python 是一种解释型语言，由 Guido 于 1991 年发布。"
- **问题**: "1991 年发布" 这个信息在上下文里**没有**，因此忠实度会扣分。

---

### 2. 💬 Answer Relevancy (答案相关性)

**含义**: 生成的答案是否**回答了用户的问题**。

**计算逻辑**:
- 将生成的答案通过大模型反向生成一个"假想问题"。
- 计算这个假想问题与原始问题的**语义相似度**。
- 如果答案跑题了，反向生成的问题就会跟原始问题差距很大，得分就低。

**理想分数**: 越接近 **1.0** 越好。

**示例**:
- **问题**: "Python 的创始人是谁？"
- **回答**: "Python 是一种解释型语言。"
- **问题**: 回答根本没提创始人，所以**跑题了**，相关性得分会很低。

---

### 3. 📚 Context Precision (上下文精确率)

**含义**: 检索到的上下文中，有多少是**真正有用的**。

**计算逻辑**:
- 检查检索到的每一段上下文，看它是否与生成正确答案有关。
- 如果检索到了 5 段，其中只有 2 段是有用的，精确率就是 2/5 = 0.4。

**理想分数**: 越接近 **1.0** 越好。说明检索器很精准，没有捞太多废料。

**示例**:
- **问题**: "什么是机器学习？"
- **检索到的上下文**:
    1. "机器学习是人工智能的一个分支..." ✅
    2. "深度学习使用神经网络..." ✅
    3. "今天天气晴朗，适合户外活动..." ❌ (无关信息)
- **精确率**: 2/3 ≈ 0.67

---

### 4. 📥 Context Recall (上下文召回率)

**含义**: 为了生成正确答案，**需要的信息是否都被检索到了**。

**计算逻辑**:
- 将标准答案 (ground_truth) 分解成多个关键论断 (claims)。
- 检查这些论断是否都能在检索到的上下文中找到支撑。
- 如果标准答案有 5 个要点，但上下文只覆盖了 3 个，召回率就是 3/5 = 0.6。

**理想分数**: 越接近 **1.0** 越好。说明检索器覆盖全面，关键信息一个都没漏。

**示例**:
- **标准答案**: "Python 由 Guido 创建，是解释型语言，首版于 1991 年发布。"
- **检索到的上下文**: "Python 是一种解释型语言，由 Guido 于 1989 年开始开发。"
- **问题**: "首版于 1991 年发布" 这个信息**没有被召回**，召回率会低于 1.0。

---

## 🛠️ 如何使用评估脚本

### 前置条件

1.  **安装依赖**:
    ```bash
    pip install ragas datasets langchain-community
    ```

2.  **启动 Ollama 服务** (如果使用本地模型):
    ```bash
    ollama serve
    # 确保已下载模型
    ollama pull qwen2.5:7b
    ```

### 运行评估

在项目根目录执行：

```bash
python rag_qa/rag_assesment/rag_as.py
```

### 输出解读

脚本运行后会打印类似以下的评估摘要：

```
============================================================
📊 Ragas 评估结果摘要
============================================================
🟢 faithfulness: 0.8521 (优秀)
🟡 answer_relevancy: 0.7234 (良好)
🟢 context_precision: 0.8100 (优秀)
🟠 context_recall: 0.5678 (一般)
============================================================
```

**图标含义**:
- 🟢 优秀 (≥ 0.8)
- 🟡 良好 (0.6 ~ 0.8)
- 🟠 一般 (0.4 ~ 0.6)
- 🔴 需改进 (< 0.4)

详细的每条数据评估结果会保存到 `ragas_evaluation_result.csv`。

---

## 📈 如何根据结果优化 RAG 系统？

| 指标低 | 优化方向 |
| :--- | :--- |
| **Faithfulness 低** | 大模型"瞎编"严重。尝试：更换更稳定的模型、调低 temperature、在 Prompt 中强调"仅根据上下文回答"。 |
| **Answer Relevancy 低** | 答案跑题。尝试：优化 Prompt 模板、增加 Query 重写逻辑、使用更强的 LLM。 |
| **Context Precision 低** | 检索到太多无关内容。尝试：调整向量库的 Top-K 参数、增加 Re-Ranker、优化 Embedding 模型。 |
| **Context Recall 低** | 关键信息没捞到。尝试：增大 Top-K、使用混合检索 (Dense + Sparse)、优化文档切分策略。 |

---

## 📂 相关文件

- **评估脚本**: `rag_qa/rag_assesment/rag_as.py`
- **评估数据**: `rag_qa/rag_assesment/rag_evaluate_data.json`
- **结果输出**: `rag_qa/rag_assesment/ragas_evaluation_result.csv` (运行后生成)

---

## 🔗 参考资料

- [Ragas 官方文档](https://docs.ragas.io/)
- [Ragas GitHub 仓库](https://github.com/explodinggradients/ragas)
