# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ä¿®å¤è„šæœ¬
ä¿®å¤é¡¹ç›®ä¸­çš„ç¡¬ç¼–ç è·¯å¾„å’ŒLangChainç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import os
import re
import shutil
from datetime import datetime

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# å¤‡ä»½ç›®å½•
BACKUP_DIR = os.path.join(PROJECT_ROOT, 'backup_before_fix')

# éœ€è¦ä¿®å¤çš„æ–‡ä»¶åˆ—è¡¨
FILES_TO_FIX = [
    {
        'path': 'rag_qa/edu_text_spliter/edu_model_text_spliter.py',
        'fixes': [
            {
                'type': 'hardcoded_path',
                'line_number': 24,
                'old': r"model=r'D:\workspace\workspace_python\python_1022\dev07_rag\integrated_qa_system\rag_qa\models\nlp_bert_document-segmentation_chinese-base',",
                'new_lines': [
                    "        # åŠ¨æ€è·å–æ¨¡å‹è·¯å¾„",
                    "        current_dir = os.path.dirname(os.path.abspath(__file__))",
                    "        rag_qa_path = os.path.dirname(current_dir)",
                    "        model_path = os.path.join(rag_qa_path, 'models', 'nlp_bert_document-segmentation_chinese-base')",
                    "        p = pipeline(",
                    "            task=\"document-segmentation\",",
                    "            model=model_path,",
                    "            device=\"cpu\")"
                ]
            },
            {
                'type': 'import',
                'old': 'from langchain.text_splitter import CharacterTextSplitter',
                'new': 'from langchain_text_splitters import CharacterTextSplitter'
            }
        ]
    },
    {
        'path': 'rag_qa/core/vector_store.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.docstore.document import Document',
                'new': 'from langchain_core.documents import Document'
            }
        ]
    },
    {
        'path': 'rag_qa/edu_text_spliter/edu_chinese_recursive_text_splitter.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.text_splitter import RecursiveCharacterTextSplitter',
                'new': 'from langchain_text_splitters import RecursiveCharacterTextSplitter'
            }
        ]
    },
    {
        'path': 'rag_qa/edu_document_loaders/edu_pdfloader.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.text_splitter import CharacterTextSplitter',
                'new': 'from langchain_text_splitters import CharacterTextSplitter'
            }
        ]
    },
    {
        'path': 'rag_qa/core/document_processor.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.text_splitter import MarkdownTextSplitter',
                'new': 'from langchain_text_splitters import MarkdownTextSplitter'
            }
        ]
    },
    {
        'path': 'rag_qa/core/strategy_selector.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.prompts import PromptTemplate',
                'new': 'from langchain_core.prompts import PromptTemplate'
            }
        ]
    },
    {
        'path': 'rag_qa/core/prompts.py',
        'fixes': [
            {
                'type': 'import',
                'old': 'from langchain.prompts import PromptTemplate',
                'new': 'from langchain_core.prompts import PromptTemplate'
            }
        ]
    }
]


def create_backup():
    """åˆ›å»ºå¤‡ä»½"""
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_subdir = os.path.join(BACKUP_DIR, f'backup_{timestamp}')
    os.makedirs(backup_subdir)
    
    print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½ç›®å½•: {backup_subdir}")
    
    for file_info in FILES_TO_FIX:
        file_path = os.path.join(PROJECT_ROOT, file_info['path'])
        if os.path.exists(file_path):
            # ä¿æŒç›®å½•ç»“æ„
            rel_dir = os.path.dirname(file_info['path'])
            backup_file_dir = os.path.join(backup_subdir, rel_dir)
            os.makedirs(backup_file_dir, exist_ok=True)
            
            backup_file_path = os.path.join(backup_subdir, file_info['path'])
            shutil.copy2(file_path, backup_file_path)
            print(f"  âœ… å·²å¤‡ä»½: {file_info['path']}")
    
    return backup_subdir


def fix_import_statement(file_path, old_import, new_import):
    """ä¿®å¤å¯¼å…¥è¯­å¥"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    if old_import in content:
        content = content.replace(old_import, new_import)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    return False


def fix_hardcoded_path(file_path, old_line, new_lines):
    """ä¿®å¤ç¡¬ç¼–ç è·¯å¾„"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # æŸ¥æ‰¾åŒ…å«æ—§ä»£ç çš„è¡Œ
    for i, line in enumerate(lines):
        if old_line.strip() in line:
            # æ‰¾åˆ°èµ·å§‹ä½ç½®(pipelineè°ƒç”¨å¼€å§‹)
            # éœ€è¦æ›¿æ¢ä» p = pipeline åˆ° device="cpu") çš„æ•´ä¸ªå—
            start_idx = i
            # å‘ä¸ŠæŸ¥æ‰¾ p = pipeline çš„ä½ç½®
            while start_idx > 0 and 'p = pipeline' not in lines[start_idx]:
                start_idx -= 1
            
            # å‘ä¸‹æŸ¥æ‰¾ç»“æŸä½ç½®
            end_idx = i
            while end_idx < len(lines) and 'device="cpu")' not in lines[end_idx]:
                end_idx += 1
            
            # è·å–ç¼©è¿›
            indent = len(lines[start_idx]) - len(lines[start_idx].lstrip())
            
            # æ›¿æ¢ä»£ç å—
            new_block = [' ' * indent + line + '\n' for line in new_lines]
            lines = lines[:start_idx] + new_block + lines[end_idx+1:]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            return True
    
    return False


def apply_fixes():
    """åº”ç”¨æ‰€æœ‰ä¿®å¤"""
    print("\nğŸ”§ å¼€å§‹ä¿®å¤æ–‡ä»¶...\n")
    
    fixed_count = 0
    failed_count = 0
    
    for file_info in FILES_TO_FIX:
        file_path = os.path.join(PROJECT_ROOT, file_info['path'])
        
        if not os.path.exists(file_path):
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_info['path']}")
            failed_count += 1
            continue
        
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_info['path']}")
        
        for fix in file_info['fixes']:
            try:
                if fix['type'] == 'import':
                    if fix_import_statement(file_path, fix['old'], fix['new']):
                        print(f"  âœ… å·²ä¿®å¤å¯¼å…¥: {fix['old'][:50]}...")
                        fixed_count += 1
                    else:
                        print(f"  âš ï¸  æœªæ‰¾åˆ°: {fix['old'][:50]}...")
                
                elif fix['type'] == 'hardcoded_path':
                    if fix_hardcoded_path(file_path, fix['old'], fix['new_lines']):
                        print(f"  âœ… å·²ä¿®å¤ç¡¬ç¼–ç è·¯å¾„")
                        fixed_count += 1
                    else:
                        print(f"  âš ï¸  æœªæ‰¾åˆ°ç¡¬ç¼–ç è·¯å¾„")
            
            except Exception as e:
                print(f"  âŒ ä¿®å¤å¤±è´¥: {e}")
                failed_count += 1
    
    return fixed_count, failed_count


def verify_fixes():
    """éªŒè¯ä¿®å¤ç»“æœ"""
    print("\nğŸ” éªŒè¯ä¿®å¤ç»“æœ...\n")
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ—§çš„å¯¼å…¥
    old_patterns = [
        'from langchain.docstore.document import',
        'from langchain.text_splitter import',
        'from langchain.prompts import',
        r"r'D:\\workspace\\workspace_python"
    ]
    
    issues_found = []
    
    for file_info in FILES_TO_FIX:
        file_path = os.path.join(PROJECT_ROOT, file_info['path'])
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for pattern in old_patterns:
                if pattern in content:
                    issues_found.append(f"{file_info['path']}: ä»åŒ…å« '{pattern}'")
    
    if issues_found:
        print("âš ï¸  å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues_found:
            print(f"  - {issue}")
        return False
    else:
        print("âœ… æ‰€æœ‰ä¿®å¤å·²æˆåŠŸåº”ç”¨!")
        return True


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("  è‡ªåŠ¨ä¿®å¤å·¥å…· - ç¡¬ç¼–ç è·¯å¾„å’ŒLangChainå…¼å®¹æ€§".center(60))
    print("="*60 + "\n")
    
    # åˆ›å»ºå¤‡ä»½
    backup_dir = create_backup()
    print(f"\nâœ… å¤‡ä»½å®Œæˆ: {backup_dir}\n")
    
    # åº”ç”¨ä¿®å¤
    fixed, failed = apply_fixes()
    
    # éªŒè¯
    success = verify_fixes()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("  ä¿®å¤æ€»ç»“".center(60))
    print("="*60)
    print(f"âœ… æˆåŠŸä¿®å¤: {fixed} é¡¹")
    if failed > 0:
        print(f"âŒ å¤±è´¥: {failed} é¡¹")
    print(f"ğŸ“¦ å¤‡ä»½ä½ç½®: {backup_dir}")
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰é—®é¢˜å·²ä¿®å¤!")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ 'python check_config.py' éªŒè¯é…ç½®")
        print("2. å®‰è£…ä¾èµ–: pip install langchain-core langchain-text-splitters")
        print("3. å¯åŠ¨åº”ç”¨: python app.py")
    else:
        print("\nâš ï¸  éƒ¨åˆ†é—®é¢˜æœªå®Œå…¨ä¿®å¤,è¯·æ‰‹åŠ¨æ£€æŸ¥")
    
    print("\n" + "="*60 + "\n")


if __name__ == "__main__":
    main()
